{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiki-taka time! \n",
    "\n",
    "---\n",
    "\n",
    "This notebook uses the Unity SoccerTwos environment, where two teams of two players each contend against each other, and the Actor Critic framework with Proximal Policy Optimization to teach the agents how to win a soccer game! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting familiar with UnityEnvironments: Soccer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring UnityEnvironments: Soccer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment! Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we obtain separate brains for the striker and the goalie agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the environment\n",
    "env = UnityEnvironment(file_name=\"Soccer.app\")\n",
    "# Print the brain names\n",
    "print(env.brain_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two brains:\n",
    "# 1. set the goalie brain\n",
    "g_brain_name = env.brain_names[0]\n",
    "g_brain = env.brains[g_brain_name]\n",
    "# 2. set the striker brain\n",
    "s_brain_name = env.brain_names[1]\n",
    "s_brain = env.brains[s_brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shed more light on the environment by printing some relevant info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=True)\n",
    "# Goalie info\n",
    "num_g_agents = len(env_info[g_brain_name].agents)\n",
    "print('Number of goalie agents:', num_g_agents)\n",
    "g_action_size = g_brain.vector_action_space_size\n",
    "print('Number of goalie actions:', g_action_size)\n",
    "g_states = env_info[g_brain_name].vector_observations\n",
    "g_state_size = g_states.shape[1]\n",
    "print('There are {} goalie agents. Each receives a state with length: {}'.format(g_states.shape[0], g_state_size))\n",
    "# Striker info\n",
    "num_s_agents = len(env_info[s_brain_name].agents)\n",
    "print('Number of striker agents:', num_s_agents)\n",
    "s_action_size = s_brain.vector_action_space_size\n",
    "print('Number of striker actions:', s_action_size)\n",
    "s_states = env_info[s_brain_name].vector_observations\n",
    "s_state_size = s_states.shape[1]\n",
    "print('There are {} striker agents. Each receives a state with length: {}'.format(s_states.shape[0], s_state_size))\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Small simulation: sample actions in the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Python API to control the agents and receive feedback from the environment. We will watch the agents' performance, as they select actions at random with each time step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the environment\n",
    "env = UnityEnvironment(file_name=\"Soccer.app\")\n",
    "\n",
    "for i in range(2):                                         # play game for 2 episodes\n",
    "    env_info = env.reset(train_mode=False)                 # reset the environment    \n",
    "    g_states = env_info[g_brain_name].vector_observations  # get initial state (goalies)\n",
    "    s_states = env_info[s_brain_name].vector_observations  # get initial state (strikers)\n",
    "    g_scores = np.zeros(num_g_agents)                      # initialize the score (goalies)\n",
    "    s_scores = np.zeros(num_s_agents)                      # initialize the score (strikers)\n",
    "    while True:\n",
    "        # select actions and send to environment\n",
    "        g_actions = np.random.randint(g_action_size, size=num_g_agents)\n",
    "        s_actions = np.random.randint(s_action_size, size=num_s_agents)\n",
    "        actions = dict(zip([g_brain_name, s_brain_name], [g_actions, s_actions]))\n",
    "        env_info = env.step(actions)\n",
    "        \n",
    "        # get next actor_states\n",
    "        g_next_states = env_info[g_brain_name].vector_observations\n",
    "        s_next_states = env_info[s_brain_name].vector_observations\n",
    "        \n",
    "        # get reward and update scores\n",
    "        g_rewards = env_info[g_brain_name].rewards\n",
    "        s_rewards = env_info[s_brain_name].rewards\n",
    "        g_scores += g_rewards\n",
    "        s_scores += s_rewards\n",
    "        \n",
    "        # check if episode finished\n",
    "        done = np.any(env_info[g_brain_name].local_done)\n",
    "        \n",
    "        # roll over actor_states to next time step\n",
    "        g_states = g_next_states\n",
    "        s_states = s_next_states\n",
    "        \n",
    "        # exit loop if episode finished\n",
    "        if done:\n",
    "            break\n",
    "    print('Scores from episode {}: {} (goalies), {} (strikers)'.format(i+1, g_scores, s_scores))\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time for training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning to be better programmers :) we use classes for cleaner coding, so all the magic is in the corresponding python files. Here we provide a short description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Actor Critic Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Actor* receives his *own state space* and outputs:\n",
    "* an action, \n",
    "* the log probability of that action (to be used later in calculating the advantage ratio), and \n",
    "* the entropy of the probability distribution (higher entropy, more uncertainty). \n",
    "The entropy acts as *noise* in the *loss function*. Intuitively, it urges the agent to try more random actions initially, so as to not get stuck in an action that fares well short-term, but is not optimal in the long-term. That is, it helps avoid local minima. \n",
    "\n",
    "The *Critic* receives the *combined state space of all agents* on the field and outputs the expected total reward for an action given that state. \n",
    "This value is compared to the actual total reward from an actor's action, and it will tell us how much better the chosen action is compared to the average likely reward. This is called the *advantage*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on the distributions function:\n",
    "\n",
    "It is not possible to have the actor simply output a softmax distribution of action probabilities and then choose an action off a random sampling of those probabilities. Neural networks cannot directly backpropagate through random samples. PyTorch and Tensorflow offer a [distribution function](https://pytorch.org/docs/stable/distributions.html) to solve this that makes the action selection differentiable. The actor passes the softmax output through this distribution function to select the action and then backpropagation can occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definitions are found in the [actor_critic_models.py](./utils/actor_critic_models.py) under the classes `Actor` and `Critic`. Each class has typical neural network functions `forward`, `load`, and `checkpoint`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a class `Memory` that contains the stored experiences for training. For each episode we store `[actor_states, critic_states, actions, log_probabilities, rewards]` for each step. The definitions are found in the [memory.py](./utils/memory.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Agent` class is a wrapper for each agent, and associates an actor model and the experiences of the actor from the running episodes. The definitions are found in the [agent.py](./utils/agent.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Optimizer` class is used by each agent and contains the actor model, the critic model, and essentially performs the learning task based on the PPO loss function. The definitions are found in the [optimizer.py](./utils/optimizer.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the `Trainer` class is a wrapper for training and testing on the whole environment for each agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's game time!\n",
    "Putting all the above together we start the environment, create the agents and optimizers, and call our coach to train and test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "from utils.agent import Agent\n",
    "from utils.actor_critic_models import Actor, Critic\n",
    "from utils.optimizer import Optimizer\n",
    "from utils.trainer import Trainer\n",
    "\n",
    "############ ENVIRONMENT ############ \n",
    "# Start the environment\n",
    "env = UnityEnvironment(file_name=\"Soccer.app\", no_graphics=True)\n",
    "# Get info: set the brains\n",
    "g_brain_name = env.brain_names[0]\n",
    "g_brain = env.brains[g_brain_name]\n",
    "s_brain_name = env.brain_names[1]\n",
    "s_brain = env.brains[s_brain_name]\n",
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=True)\n",
    "# Goalie info\n",
    "num_g_agents = len(env_info[g_brain_name].agents)\n",
    "g_action_size = g_brain.vector_action_space_size\n",
    "g_states = env_info[g_brain_name].vector_observations\n",
    "g_state_size = g_states.shape[1]\n",
    "# Striker info\n",
    "num_s_agents = len(env_info[s_brain_name].agents)\n",
    "s_action_size = s_brain.vector_action_space_size\n",
    "s_states = env_info[s_brain_name].vector_observations\n",
    "s_state_size = s_states.shape[1]\n",
    "\n",
    "############ HYPERPARAMETERS ############\n",
    "# Set hyperparameters\n",
    "N_EPOCHS = 6000\n",
    "N_STEP = 8\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.995\n",
    "EPSILON = 0.1\n",
    "ENTROPY_WEIGHT = 0.001\n",
    "GRADIENT_CLIP = 0.5\n",
    "GOALIE_LR = 8e-5\n",
    "STRIKER_LR = 8e-5\n",
    "\n",
    "# Set torch.device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "############ ACTOR-CRITIC MODELS ############\n",
    "# Set checkpoints for models\n",
    "CHECKPOINT_GOALIE_ACTOR = './checkpoint_g_actor.pth'\n",
    "CHECKPOINT_GOALIE_CRITIC = './checkpoint_g_critic.pth'\n",
    "CHECKPOINT_STRIKER_ACTOR = './checkpoint_s_actor.pth'\n",
    "CHECKPOINT_STRIKER_CRITIC = './checkpoint_s_critic.pth'\n",
    "# Actors and Critics\n",
    "GOALIE_0_KEY = 0\n",
    "STRIKER_0_KEY = 0\n",
    "GOALIE_1_KEY = 1\n",
    "STRIKER_1_KEY = 1\n",
    "# Goalie Actor-Critic\n",
    "goalie_actor_model = Actor(g_state_size, g_action_size, CHECKPOINT_GOALIE_ACTOR).to(DEVICE)\n",
    "goalie_critic_model = Critic(g_state_size + s_state_size + g_state_size + s_state_size, CHECKPOINT_GOALIE_CRITIC).to(DEVICE)\n",
    "goalie_optim = optim.Adam(list(goalie_actor_model.parameters()) + list(goalie_critic_model.parameters()), lr=GOALIE_LR)\n",
    "goalie_actor_model.load()\n",
    "goalie_critic_model.load()\n",
    "# Striker Actor-Critic\n",
    "striker_actor_model = Actor(s_state_size, s_action_size, CHECKPOINT_STRIKER_ACTOR).to(DEVICE)\n",
    "striker_critic_model = Critic(s_state_size + g_state_size + s_state_size + g_state_size, CHECKPOINT_STRIKER_CRITIC).to(DEVICE)\n",
    "striker_optim = optim.Adam(list(striker_actor_model.parameters()) + list(striker_critic_model.parameters()), lr=STRIKER_LR)\n",
    "striker_actor_model.load()\n",
    "striker_critic_model.load()\n",
    "\n",
    "############ AGENTS ############\n",
    "goalie_0 = Agent(DEVICE, GOALIE_0_KEY, goalie_actor_model, N_STEP)\n",
    "goalie_optimizer = Optimizer(DEVICE, goalie_actor_model, goalie_critic_model, goalie_optim, N_STEP, BATCH_SIZE, GAMMA, EPSILON, ENTROPY_WEIGHT, GRADIENT_CLIP)\n",
    "striker_0 = Agent(DEVICE, STRIKER_0_KEY, striker_actor_model, N_STEP)\n",
    "striker_optimizer = Optimizer(DEVICE, striker_actor_model, striker_critic_model, striker_optim, N_STEP, BATCH_SIZE, GAMMA, EPSILON, ENTROPY_WEIGHT, GRADIENT_CLIP)\n",
    "\n",
    "############ TRAINING ############\n",
    "# Get the coach!\n",
    "coach = Trainer(env, DEVICE, N_EPOCHS, goalie_0, goalie_optimizer, striker_0, striker_optimizer)\n",
    "# Train the team\n",
    "coach.ppo_train(GOALIE_1_KEY, STRIKER_1_KEY)\n",
    "# Test the team\n",
    "coach.test(GOALIE_1_KEY, STRIKER_1_KEY)\n",
    "\n",
    "############ THE END ############\n",
    "# Close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
