{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiki-taka time! \n",
    "\n",
    "---\n",
    "\n",
    "This notebook uses the Unity SoccerTwos environment, where two teams of two players each contend against each other, and the Actor Critic framework with Proximal Policy Optimization to teach the agents how to win a soccer game! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use PyTorch to implement the neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.distributions as distributions\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "# We import the UnityEnvironment\n",
    "from unityagents import UnityEnvironment\n",
    "# and some other relevant packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import datetime\n",
    "import pytz\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function to get the local time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(format):\n",
    "    utc_now = pytz.utc.localize(datetime.datetime.utcnow())\n",
    "    pst_now = utc_now.astimezone(pytz.timezone(\"America/Los_Angeles\"))\n",
    "    return pst_now.strftime(format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring UnityEnvironments: Soccer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment! Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we obtain separate brains for the striker and the goalie agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the environment\n",
    "env = UnityEnvironment(file_name=\"Soccer.app\")\n",
    "# Print the brain names\n",
    "print(env.brain_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two brains:\n",
    "# 1. set the goalie brain\n",
    "g_brain_name = env.brain_names[0]\n",
    "g_brain = env.brains[g_brain_name]\n",
    "# 2. set the striker brain\n",
    "s_brain_name = env.brain_names[1]\n",
    "s_brain = env.brains[s_brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shed more light on the environment by printing some relevant info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=True)\n",
    "# Goalie info\n",
    "num_g_agents = len(env_info[g_brain_name].agents)\n",
    "print('Number of goalie agents:', num_g_agents)\n",
    "g_action_size = g_brain.vector_action_space_size\n",
    "print('Number of goalie actions:', g_action_size)\n",
    "g_states = env_info[g_brain_name].vector_observations\n",
    "g_state_size = g_states.shape[1]\n",
    "print('There are {} goalie agents. Each receives a state with length: {}'.format(g_states.shape[0], g_state_size))\n",
    "# Striker info\n",
    "num_s_agents = len(env_info[s_brain_name].agents)\n",
    "print('Number of striker agents:', num_s_agents)\n",
    "s_action_size = s_brain.vector_action_space_size\n",
    "print('Number of striker actions:', s_action_size)\n",
    "s_states = env_info[s_brain_name].vector_observations\n",
    "s_state_size = s_states.shape[1]\n",
    "print('There are {} striker agents. Each receives a state with length: {}'.format(s_states.shape[0], s_state_size))\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small simulation: sample actions in the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Python API to control the agents and receive feedback from the environment. We will watch the agents' performance, as they select actions at random with each time step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the environment\n",
    "env = UnityEnvironment(file_name=\"Soccer.app\")\n",
    "\n",
    "for i in range(2):                                         # play game for 2 episodes\n",
    "    env_info = env.reset(train_mode=False)                 # reset the environment    \n",
    "    g_states = env_info[g_brain_name].vector_observations  # get initial state (goalies)\n",
    "    s_states = env_info[s_brain_name].vector_observations  # get initial state (strikers)\n",
    "    g_scores = np.zeros(num_g_agents)                      # initialize the score (goalies)\n",
    "    s_scores = np.zeros(num_s_agents)                      # initialize the score (strikers)\n",
    "    while True:\n",
    "        # select actions and send to environment\n",
    "        g_actions = np.random.randint(g_action_size, size=num_g_agents)\n",
    "        s_actions = np.random.randint(s_action_size, size=num_s_agents)\n",
    "        actions = dict(zip([g_brain_name, s_brain_name], [g_actions, s_actions]))\n",
    "        env_info = env.step(actions)\n",
    "        \n",
    "        # get next actor_states\n",
    "        g_next_states = env_info[g_brain_name].vector_observations\n",
    "        s_next_states = env_info[s_brain_name].vector_observations\n",
    "        \n",
    "        # get reward and update scores\n",
    "        g_rewards = env_info[g_brain_name].rewards\n",
    "        s_rewards = env_info[s_brain_name].rewards\n",
    "        g_scores += g_rewards\n",
    "        s_scores += s_rewards\n",
    "        \n",
    "        # check if episode finished\n",
    "        done = np.any(env_info[g_brain_name].local_done)\n",
    "        \n",
    "        # roll over actor_states to next time step\n",
    "        g_states = g_next_states\n",
    "        s_states = s_next_states\n",
    "        \n",
    "        # exit loop if episode finished\n",
    "        if done:\n",
    "            break\n",
    "    print('Scores from episode {}: {} (goalies), {} (strikers)'.format(i+1, g_scores, s_scores))\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time for training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup the Actor Critic Networks\n",
    "\n",
    "The *Actor* receives his *own state space* and outputs:\n",
    "* an action, \n",
    "* the log probability of that action (to be used later in calculating the advantage ratio), and \n",
    "* the entropy of the probability distribution (higher entropy, more uncertainty). \n",
    "The entropy acts as *noise* in the *loss function*. Intuitively, it urges the agent to try more random actions initially, so as to not get stuck in an action that fares well short-term, but is not optimal in the long-term. That is, it helps avoid local minima. \n",
    "\n",
    "The *Critic* receives the *combined state space of all agents* on the field and outputs the expected total reward for an action given that state. \n",
    "This value is compared to the actual total reward from an actor's action, and it will tell us how much better the chosen action is compared to the average likely reward. This is called the *advantage*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on the distributions function:\n",
    "\n",
    "It is not possible to have the actor simply output a softmax distribution of action probabilities and then choose an action off a random sampling of those probabilities. Neural networks cannot directly backpropagate through random samples. PyTorch and Tensorflow offer a [distribution function](https://pytorch.org/docs/stable/distributions.html) to solve this that makes the action selection differentiable. The actor passes the softmax output through this distribution function to select the action and then backpropagation can occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we provide the definitions for the actor and the critic networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_fc_layer(layer, weight_scale=1.0, bias=0.0):\n",
    "    \"\"\"\n",
    "    Initializes a fully connected layer. \n",
    "\n",
    "    Arguments:\n",
    "      layer - torch.nn.<layer> module\n",
    "      w_scale - float\n",
    "    Outputs:\n",
    "      Initialized layer\n",
    "    \"\"\"\n",
    "    # 'nn.init.orthogonal_' fills the 'layer.weight.data' (Tensor) with a (semi) orthogonal matrix, \n",
    "    # see Exact solutions to the nonlinear dynamics of learning in deep linear neural networks - Saxe, A. et al. (2013). \n",
    "    nn.init.orthogonal_(layer.weight.data, gain=weight_scale)\n",
    "    # layer.weight.data.mul_(weight_scale)\n",
    "    # 'nn.init.constant_' fills the 'layer.bias.data' (Tensor) with the 'bias' value.\n",
    "    nn.init.constant_(layer.bias.data, bias)\n",
    "    return layer\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    # Chosen architecture:\n",
    "    # input -> fc1 -> relu -> fc2 -> relu -> fc3 -> softmax -> output\n",
    "\n",
    "    # Initialize\n",
    "    def __init__(self, state_size, action_size, hidden_0=256, hidden_1=128):\n",
    "        super(Actor, self).__init__()\n",
    "        # We initialize 3 fully connected layers. \n",
    "        self.fc1 = initialize_fc_layer(nn.Linear(state_size, hidden_0))\n",
    "        self.fc2 = initialize_fc_layer(nn.Linear(hidden_0, hidden_1))\n",
    "        self.fc3 = initialize_fc_layer(nn.Linear(hidden_1, action_size))\n",
    "    \n",
    "    # Forward propagation\n",
    "    def forward(self, x, action=None):\n",
    "        # Input x\n",
    "        # -> fc1 -> relu\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # -> fc2 -> relu\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # -> fc3 -> softmax\n",
    "        probs = F.softmax(self.fc3(x), dim=1)\n",
    "        \n",
    "        # Create Categorical distribution based on the\n",
    "        # probabilities out of the softmax. \n",
    "        dist = distributions.Categorical(probs)\n",
    "\n",
    "        # If no action provided, sample randomly\n",
    "        # based on the distribution from the nn output.\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        \n",
    "        # Compute the log-probability density/mass function \n",
    "        # evaluated at the 'action' value.\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action, log_prob, dist.entropy()\n",
    "\n",
    "    # Load from checkpoint\n",
    "    def load(self, checkpoint):        \n",
    "        if os.path.isfile(checkpoint):\n",
    "            self.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "    # Save to checkpoint\n",
    "    def checkpoint(self, checkpoint):\n",
    "        torch.save(self.state_dict(), checkpoint)\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    # Chosen architecture:\n",
    "    # input -> fc1 -> relu -> fc2 -> relu -> fc3 -> output\n",
    "\n",
    "    # Initialize\n",
    "    def __init__(self, state_size, hidden_0=256, hidden_1=128):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.fc1 = initialize_fc_layer(nn.Linear(state_size*4, hidden_0))\n",
    "        self.fc2 = initialize_fc_layer(nn.Linear(hidden_0, hidden_1))\n",
    "        self.fc3 = initialize_fc_layer(nn.Linear(hidden_1, 1))\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self, x):\n",
    "        # Input x\n",
    "        # -> fc1 -> relu\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # -> fc2 -> relu\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # -> fc3\n",
    "        value = self.fc3(x)\n",
    "        return value\n",
    "\n",
    "    # Load from checkpoint\n",
    "    def load(self, checkpoint):        \n",
    "        if os.path.isfile(checkpoint):\n",
    "            self.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "    # Save to checkpoint\n",
    "    def checkpoint(self, checkpoint):\n",
    "        torch.save(self.state_dict(), checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy improvement\n",
    "\n",
    "The learning process takes the experiences from the agents playing one full soccer game. This is either play until a goal was scored or 600 time steps passed and the game was terminated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting up classes for training\n",
    "Learning to be better programmers :) we use classes for cleaner coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`class Memory` contains the stored experiences for training/evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "        self.experience = namedtuple('Experience', field_names=['actor_state', 'critic_state', 'action', 'log_prob', 'reward'])\n",
    "\n",
    "    def add(self, actor_state, critic_state, action, log_prob, reward):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"        \n",
    "        exp = self.experience(actor_state, critic_state, action, log_prob, reward)\n",
    "        self.memory.append(exp)\n",
    "\n",
    "    def experiences(self, clear=True):\n",
    "        \"\"\"Return experiences stored in memory\"\"\"\n",
    "        # Number of experiences is the length of self.memory. \n",
    "        n_exp = len(self.memory)\n",
    "        # For each exp in self.memory, stack the\n",
    "        # (actor_states, critic_states, actions, log_probabilities, rewards)\n",
    "        actor_states = np.vstack([exp.actor_state for exp in self.memory if exp is not None])\n",
    "        critic_states = np.vstack([exp.critic_state for exp in self.memory if exp is not None])\n",
    "        actions = np.vstack([exp.action for exp in self.memory if exp is not None])\n",
    "        log_probs = np.vstack([exp.log_prob for exp in self.memory if exp is not None])\n",
    "        rewards = np.vstack([exp.reward for exp in self.memory if exp is not None])\n",
    "\n",
    "        # Clear memory after returning experiences.\n",
    "        if clear:\n",
    "            self.memory.clear()\n",
    "\n",
    "        return actor_states, critic_states, actions, log_probs, rewards, n_exp\n",
    "    \n",
    "    def delete(self, i):\n",
    "        del self.memory[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`class Agent` is a wrapper for each agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from memory import Memory\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, device, key, actor_model, n_step):\n",
    "        # Set device\n",
    "        self.device = device\n",
    "        # Set key\n",
    "        self.KEY = key\n",
    "        # Set neural model\n",
    "        self.actor_model = actor_model   \n",
    "        # MEMORY\n",
    "        self.memory = Memory()\n",
    "        # Set number of steps\n",
    "        self.N_STEP = n_step\n",
    "\n",
    "\n",
    "    #get an action from the actor for each step of game play (inference/eval only)\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        self.actor_model.eval()\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _ = self.actor_model(state)\n",
    "        self.actor_model.train()\n",
    "        action = action.cpu().detach().numpy().item()\n",
    "        log_prob = log_prob.cpu().detach().numpy().item()\n",
    "        return action, log_prob\n",
    "\n",
    "    def step(self, actor_state, critic_state, action, log_prob, reward):\n",
    "        self.memory.add(actor_state, critic_state, action, log_prob, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create another class for our `Optimizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "\n",
    "    def __init__(self, device, actor_model, critic_model, optimizer, epochs, n_step, batch_size, gamma, epsilon, entropy_weight, gradient_clip):\n",
    "        # Set device\n",
    "        self.device = device \n",
    "\n",
    "        # Set neural nets\n",
    "        self.actor_model = actor_model\n",
    "        self.critic_model = critic_model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Set hyperparameters\n",
    "        self.epochs = epochs\n",
    "        self.n_step = n_step\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.entropy_weight = entropy_weight\n",
    "        self.gradient_clip = gradient_clip  \n",
    "\n",
    "    def learn(self, memory):\n",
    "        # Extract experiences from memory:\n",
    "        actor_states, critic_states, actions, log_probs, rewards, n_exp = memory.experiences()\n",
    "\n",
    "        # Discounts: gamma, gamma^2, gamma^3, ...\n",
    "        discounts = self.gamma**np.arange(n_exp)\n",
    "        # Discount the rewards of the episode\n",
    "        discounted_rewards = rewards.squeeze(1) * discounts\n",
    "        # Compute the total discounted reward for the episode\n",
    "        rewards_future = discounted_rewards[::-1].cumsum(axis=0)[::-1]\n",
    "        \n",
    "        # Setup torch tensors\n",
    "        actor_states = torch.from_numpy(actor_states).float().to(self.device)\n",
    "        critic_states = torch.from_numpy(critic_states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device).squeeze(1)\n",
    "        log_probs = torch.from_numpy(log_probs).float().to(self.device).squeeze(1)\n",
    "        rewards = torch.from_numpy(rewards_future.copy()).float().to(self.device)\n",
    "        \n",
    "        \"\"\"\n",
    "        We want the agent to take actions which achieve the greatest reward compared to the average expected reward \n",
    "        for that state (as estimated by our critic). We compute the advantage function\n",
    "        below and normalize it to improve training.\n",
    "        \"\"\"\n",
    "        # Get critic values detached from the training process (eval/inference only)\n",
    "        self.critic_model.eval()\n",
    "        with torch.no_grad():\n",
    "            values = self.critic_model(critic_states).detach()\n",
    "        self.critic_model.train()\n",
    "\n",
    "        # Get advantages\n",
    "        advantages = (rewards - values.squeeze()).detach()\n",
    "        advantages_normalized = (advantages - advantages.mean()) / (advantages.std() + 1.0e-10)\n",
    "        advantages_normalized = torch.tensor(advantages_normalized).float().to(self.device)\n",
    "\n",
    "        \"\"\"\n",
    "        Each epoch has a set of experiences (n_exp). \n",
    "        We take a random mini-batch of experiences to train on.\n",
    "        \"\"\"\n",
    "        batches = BatchSampler(SubsetRandomSampler(range(0, n_exp)), self.batch_size, drop_last=False)\n",
    "        losses = []\n",
    "        for batch_indices in batches:\n",
    "            batch_indices = torch.tensor(batch_indices).long().to(self.device)\n",
    "\n",
    "            # Get data from the batch\n",
    "            sampled_actor_states = actor_states[batch_indices]\n",
    "            sampled_critic_states = critic_states[batch_indices]\n",
    "            sampled_actions = actions[batch_indices]\n",
    "            sampled_log_probs = log_probs[batch_indices]\n",
    "            sampled_rewards = rewards[batch_indices]\n",
    "            sampled_advantages = advantages_normalized[batch_indices]\n",
    "\n",
    "            # Get new probability of each action given the state and latest actor policy\n",
    "            _, new_log_probs, entropies = self.actor_model(sampled_actor_states, sampled_actions)\n",
    "\n",
    "            # Compute ratio of how much more likely is the new action choice vs. old choice \n",
    "            # according to the updated actor\n",
    "            ratio = (new_log_probs - sampled_log_probs).exp()\n",
    "\n",
    "            # Compute PPO loss\n",
    "            \"\"\"\n",
    "            The clipping function makes sure that we don't update our weights too much when we find a much better \n",
    "            choice. This makes sure we do not charge in a false lead. \n",
    "            This is the key idea of Proximal Policy Optimization (PPO). \n",
    "            \"\"\"\n",
    "            clip = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)\n",
    "            policy_loss = torch.min( ratio * sampled_advantages, clip * sampled_advantages )\n",
    "            policy_loss = - torch.mean( policy_loss )\n",
    "\n",
    "            \"\"\"\n",
    "            Entropy regularization term steers the new policy towards equal probability of all actions, encouraging\n",
    "            exploration early on, but decreasing in importance over time. \n",
    "            \"\"\"\n",
    "            entropy = torch.mean(entropies)\n",
    "            # Get predicted future rewards to use in backpropagation to improve the critic's estimates\n",
    "            values = self.critic_model(sampled_critic_states) \n",
    "            value_loss = F.mse_loss(sampled_rewards, values.squeeze())\n",
    "\n",
    "            \"\"\"\n",
    "            The loss function combines the policy loss with value loss and adds the entropy term. PyTorch will\n",
    "            backpropagate the respective losses through to each network's parameters and optimize over time.\n",
    "            \"\"\"\n",
    "            loss = policy_loss + (0.5 * value_loss) - (entropy * self.entropy_weight)  \n",
    "\n",
    "            self.optimizer.zero_grad()                  \n",
    "            loss.backward()\n",
    "            # nn.utils.clip_grad_norm_( self.actor_model.parameters(), self.GRADIENT_CLIP )\n",
    "            # nn.utils.clip_grad_norm_( self.critic_model.parameters(), self.GRADIENT_CLIP )\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "            losses.append(loss.data)\n",
    "\n",
    "            # # some reporting to check performance\n",
    "            # if self.actor_model == striker_0_actor:\n",
    "            #     episode_loss.append(policy_loss.cpu().detach().numpy().squeeze().item())\n",
    "            #     policy_loss_value.append(policy_loss.cpu().detach().numpy().squeeze().item())\n",
    "            #     value_loss_value.append(value_loss.cpu().detach().numpy().squeeze().item())\n",
    "            #     entropy_value.append(torch.mean(entropy))\n",
    "        \n",
    "            # if self.actor_model == goalie_0_actor:\n",
    "            #     episode_loss.append(policy_loss.cpu().detach().numpy().squeeze().item())\n",
    "            #     policy_loss_value_g.append(policy_loss.cpu().detach().numpy().squeeze().item())\n",
    "            #     value_loss_value_g.append(value_loss.cpu().detach().numpy().squeeze().item())\n",
    "            #     entropy_value_g.append(torch.mean(entropy))\n",
    "\n",
    "        self.epsilon *= 1\n",
    "        self.entropy_weight *= 0.995\n",
    "\n",
    "        return np.average(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It's training time\n",
    "\n",
    "Putting all the above together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set torch.device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Start the environment\n",
    "env = UnityEnvironment(file_name=\"Soccer.app\") #no_graphis=True)\n",
    "# Get info\n",
    "# set the goalie brain\n",
    "g_brain_name = env.brain_names[0]\n",
    "g_brain = env.brains[g_brain_name]\n",
    "\n",
    "# set the striker brain\n",
    "s_brain_name = env.brain_names[1]\n",
    "s_brain = env.brains[s_brain_name]\n",
    "\n",
    "\n",
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=True)\n",
    "# Goalie info\n",
    "num_g_agents = len(env_info[g_brain_name].agents)\n",
    "g_action_size = g_brain.vector_action_space_size\n",
    "g_states = env_info[g_brain_name].vector_observations\n",
    "g_state_size = g_states.shape[1]\n",
    "# Striker info\n",
    "num_s_agents = len(env_info[s_brain_name].agents)\n",
    "s_action_size = s_brain.vector_action_space_size\n",
    "s_states = env_info[s_brain_name].vector_observations\n",
    "s_state_size = s_states.shape[1]\n",
    "\n",
    "\n",
    "# Set hyperparameters\n",
    "N_STEP = 8\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.995\n",
    "EPSILON = 0.1\n",
    "ENTROPY_WEIGHT = 0.001\n",
    "GRADIENT_CLIP = 0.5\n",
    "GOALIE_LR = 8e-5\n",
    "STRIKER_LR = 1e-4\n",
    "\n",
    "# Set checkpoints to save trained models\n",
    "CHECKPOINT_GOALIE_ACTOR = './checkpoint_goalie_actor.pth'\n",
    "CHECKPOINT_GOALIE_CRITIC = './checkpoint_goalie_critic.pth'\n",
    "CHECKPOINT_STRIKER_ACTOR = './checkpoint_striker_actor.pth'\n",
    "CHECKPOINT_STRIKER_CRITIC = './checkpoint_striker_critic.pth'\n",
    "\n",
    "# Actors and Critics\n",
    "GOALIE_0_KEY = 0\n",
    "STRIKER_0_KEY = 0\n",
    "GOALIE_1_KEY = 1\n",
    "STRIKER_1_KEY = 1\n",
    "\n",
    "# Goalie Actor-Critic\n",
    "goalie_actor_model = Actor(g_state_size, g_action_size).to(DEVICE)\n",
    "goalie_critic_model = Critic(g_state_size + s_state_size + g_state_size + s_state_size).to(DEVICE)\n",
    "goalie_optim = optim.Adam(list(goalie_actor_model.parameters()) + list(goalie_critic_model.parameters()), lr=GOALIE_LR )\n",
    "# self.optim = optim.RMSprop( list( self.actor_model.parameters() ) + list( self.critic_model.parameters() ), lr=lr, alpha=0.99, eps=1e-5 )\n",
    "goalie_actor_model.load(CHECKPOINT_GOALIE_ACTOR)\n",
    "goalie_critic_model.load(CHECKPOINT_GOALIE_CRITIC)\n",
    "\n",
    "# Striker Actor-Critic\n",
    "striker_actor_model = Actor( s_state_size, s_action_size ).to(DEVICE)\n",
    "striker_critic_model = Critic(s_state_size + g_state_size + s_state_size + g_state_size).to(DEVICE)\n",
    "striker_optim = optim.Adam(list(striker_actor_model.parameters()) + list(striker_critic_model.parameters()), lr=STRIKER_LR )\n",
    "# self.optim = optim.RMSprop( list( self.actor_model.parameters() ) + list( self.critic_model.parameters() ), lr=lr, alpha=0.99, eps=1e-5 )\n",
    "striker_actor_model.load(CHECKPOINT_STRIKER_ACTOR)\n",
    "striker_critic_model.load(CHECKPOINT_STRIKER_CRITIC)\n",
    "\n",
    "\n",
    "# Agents\n",
    "goalie_0 = Agent(DEVICE, GOALIE_0_KEY, goalie_actor_model, N_STEP)\n",
    "goalie_optimizer = Optimizer(DEVICE, goalie_actor_model, goalie_critic_model, goalie_optim, N_STEP, BATCH_SIZE, GAMMA, EPSILON, ENTROPY_WEIGHT, GRADIENT_CLIP)\n",
    "\n",
    "striker_0 = Agent(DEVICE, STRIKER_0_KEY, striker_actor_model, N_STEP)\n",
    "striker_optimizer = Optimizer(DEVICE, striker_actor_model, striker_critic_model, striker_optim, N_STEP, BATCH_SIZE, GAMMA, EPSILON, ENTROPY_WEIGHT, GRADIENT_CLIP)\n",
    "\n",
    "def ppo_train():\n",
    "    n_episodes = 5000\n",
    "    team_0_window_score = deque(maxlen=100)\n",
    "    team_0_window_score_wins = deque(maxlen=100)\n",
    "\n",
    "    team_1_window_score = deque(maxlen=100)\n",
    "    team_1_window_score_wins = deque(maxlen=100)\n",
    "\n",
    "    draws = deque(maxlen=100)\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        # Reset the environment\n",
    "        env_info = env.reset(train_mode=True)\n",
    "        # Get initial states\n",
    "        g_states = env_info[g_brain_name].vector_observations\n",
    "        s_states = env_info[s_brain_name].vector_observations\n",
    "        # Initialize scores\n",
    "        g_scores = np.zeros(num_g_agents)\n",
    "        s_scores = np.zeros(num_s_agents)\n",
    "\n",
    "        steps = 0\n",
    "        while True:\n",
    "            # Select actions and send to environment\n",
    "            action_goalie_0, log_prob_goalie_0 = goalie_0.act(g_states[goalie_0.KEY])\n",
    "            action_striker_0, log_prob_striker_0 = striker_0.act(s_states[striker_0.KEY])\n",
    "\n",
    "            # action_goalie_1, log_prob_goalie_1 = goalie_1.act( g_states[goalie_1.KEY] )\n",
    "            # action_striker_1, log_prob_striker_1 = striker_1.act( s_states[striker_1.KEY] )\n",
    "            \n",
    "            # random            \n",
    "            action_goalie_1 = np.asarray( [np.random.choice(g_action_size)] )\n",
    "            action_striker_1 = np.asarray( [np.random.choice(s_action_size)] )\n",
    "\n",
    "\n",
    "            actions_goalies = np.array( (action_goalie_0, action_goalie_1) )                                    \n",
    "            actions_strikers = np.array( (action_striker_0, action_striker_1) )\n",
    "\n",
    "            actions = dict( zip( [g_brain_name, s_brain_name], [actions_goalies, actions_strikers] ) )\n",
    "\n",
    "        \n",
    "            env_info = env.step(actions)                                                \n",
    "            # get next states\n",
    "            goalies_next_states = env_info[g_brain_name].vector_observations         \n",
    "            strikers_next_states = env_info[s_brain_name].vector_observations\n",
    "            \n",
    "            # get reward and update scores\n",
    "            goalies_rewards = env_info[g_brain_name].rewards  \n",
    "            strikers_rewards = env_info[s_brain_name].rewards\n",
    "            g_scores += goalies_rewards\n",
    "            s_scores += strikers_rewards\n",
    "                        \n",
    "            # check if episode finished\n",
    "            done = np.any(env_info[g_brain_name].local_done)\n",
    "\n",
    "            # store experiences\n",
    "            goalie_0_reward = goalies_rewards[goalie_0.KEY]\n",
    "            goalie_0.step( \n",
    "                g_states[goalie_0.KEY],\n",
    "                np.concatenate( \n",
    "                    (\n",
    "                        g_states[goalie_0.KEY],\n",
    "                        s_states[striker_0.KEY],\n",
    "                        g_states[GOALIE_1_KEY],\n",
    "                        s_states[STRIKER_1_KEY],\n",
    "                    ), axis=0 ),\n",
    "                action_goalie_0,\n",
    "                log_prob_goalie_0,\n",
    "                goalie_0_reward \n",
    "            )\n",
    "\n",
    "\n",
    "            striker_0_reward = strikers_rewards[striker_0.KEY]\n",
    "            striker_0.step(                 \n",
    "                s_states[striker_0.KEY],\n",
    "                np.concatenate( \n",
    "                    (\n",
    "                        s_states[striker_0.KEY],\n",
    "                        g_states[goalie_0.KEY],                        \n",
    "                        s_states[STRIKER_1_KEY],                 \n",
    "                        g_states[GOALIE_1_KEY]                        \n",
    "                    ), axis=0 ),               \n",
    "                action_striker_0,\n",
    "                log_prob_striker_0,\n",
    "                striker_0_reward\n",
    "            )\n",
    "\n",
    "\n",
    "            # exit loop if episode finished\n",
    "            if done:\n",
    "                break  \n",
    "\n",
    "            # roll over states to next time step\n",
    "            g_states = goalies_next_states\n",
    "            s_states = strikers_next_states\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "        # learn\n",
    "        goalie_loss = goalie_optimizer.learn(goalie_0.memory)\n",
    "        striker_loss = striker_optimizer.learn(striker_0.memory)        \n",
    "\n",
    "        goalie_actor_model.checkpoint( CHECKPOINT_GOALIE_ACTOR )   \n",
    "        goalie_critic_model.checkpoint( CHECKPOINT_GOALIE_CRITIC )    \n",
    "        striker_actor_model.checkpoint( CHECKPOINT_STRIKER_ACTOR )    \n",
    "        striker_critic_model.checkpoint( CHECKPOINT_STRIKER_CRITIC )\n",
    "\n",
    "        team_0_score = g_scores[goalie_0.KEY] + s_scores[striker_0.KEY]\n",
    "        team_0_window_score.append( team_0_score )\n",
    "        team_0_window_score_wins.append( 1 if team_0_score > 0 else 0)        \n",
    "\n",
    "        team_1_score = g_scores[GOALIE_1_KEY] + s_scores[STRIKER_1_KEY]\n",
    "        team_1_window_score.append( team_1_score )\n",
    "        team_1_window_score_wins.append( 1 if team_1_score > 0 else 0 )\n",
    "\n",
    "        draws.append( team_0_score == team_1_score )\n",
    "        \n",
    "        print('Episode: {} \\tSteps: \\t{} \\tGoalie Loss: \\t {:.10f} \\tStriker Loss: \\t {:.10f}'.format( episode + 1, steps, goalie_loss, striker_loss ))\n",
    "        print('\\tRed Wins: \\t{} \\tScore: \\t{:.5f} \\tAvg: \\t{:.2f}'.format( np.count_nonzero(team_0_window_score_wins), team_0_score, np.sum(team_0_window_score) ))\n",
    "        print('\\tBlue Wins: \\t{} \\tScore: \\t{:.5f} \\tAvg: \\t{:.2f}'.format( np.count_nonzero(team_1_window_score_wins), team_1_score, np.sum(team_1_window_score) ))\n",
    "        print('\\tDraws: \\t{}'.format( np.count_nonzero(draws) ))\n",
    "\n",
    "        if np.count_nonzero( team_0_window_score_wins ) >= 95:\n",
    "            break\n",
    "    \n",
    "\n",
    "# train the agent\n",
    "# ppo_train()\n",
    "\n",
    "# test the trained agents\n",
    "team_0_window_score = deque(maxlen=100)\n",
    "team_0_window_score_wins = deque(maxlen=100)\n",
    "\n",
    "team_1_window_score = deque(maxlen=100)\n",
    "team_1_window_score_wins = deque(maxlen=100)\n",
    "\n",
    "draws = deque(maxlen=100)\n",
    "\n",
    "for episode in range(50):                                               # play game for n episodes\n",
    "    env_info = env.reset(train_mode=False)                              # reset the environment    \n",
    "    g_states = env_info[g_brain_name].vector_observations         # get initial state (goalies)\n",
    "    s_states = env_info[s_brain_name].vector_observations        # get initial state (strikers)\n",
    "\n",
    "    g_scores = np.zeros(num_g_agents)                          # initialize the score (goalies)\n",
    "    s_scores = np.zeros(num_s_agents)                        # initialize the score (strikers)\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "        # select actions and send to environment\n",
    "        action_goalie_0, log_prob_goalie_0 = goalie_0.act( g_states[goalie_0.KEY] )\n",
    "        action_striker_0, log_prob_striker_0 = striker_0.act( s_states[striker_0.KEY] )\n",
    "\n",
    "        # action_goalie_1, log_prob_goalie_1 = goalie_1.act( g_states[goalie_1.KEY] )\n",
    "        # action_striker_1, log_prob_striker_1 = striker_1.act( s_states[striker_1.KEY] )\n",
    "        \n",
    "        # random            \n",
    "        action_goalie_1 = np.asarray( [np.random.randint(g_action_size)] )\n",
    "        action_striker_1 = np.asarray( [np.random.randint(s_action_size)] )\n",
    "\n",
    "\n",
    "        actions_goalies = np.array( (action_goalie_0, action_goalie_1) )                                    \n",
    "        actions_strikers = np.array( (action_striker_0, action_striker_1) )\n",
    "\n",
    "        actions = dict( zip( [g_brain_name, s_brain_name], [actions_goalies, actions_strikers] ) )\n",
    "\n",
    "    \n",
    "        env_info = env.step(actions)                                                \n",
    "        # get next states\n",
    "        goalies_next_states = env_info[g_brain_name].vector_observations         \n",
    "        strikers_next_states = env_info[s_brain_name].vector_observations\n",
    "        \n",
    "        # get reward and update scores\n",
    "        goalies_rewards = env_info[g_brain_name].rewards  \n",
    "        strikers_rewards = env_info[s_brain_name].rewards\n",
    "        g_scores += goalies_rewards\n",
    "        s_scores += strikers_rewards\n",
    "                    \n",
    "        # check if episode finished\n",
    "        done = np.any(env_info[g_brain_name].local_done)\n",
    "\n",
    "        # exit loop if episode finished\n",
    "        if done:\n",
    "            break  \n",
    "\n",
    "        # roll over states to next time step\n",
    "        g_states = goalies_next_states\n",
    "        s_states = strikers_next_states\n",
    "\n",
    "        steps += 1\n",
    "        \n",
    "    team_0_score = g_scores[goalie_0.KEY] + s_scores[striker_0.KEY]\n",
    "    team_0_window_score.append( team_0_score )\n",
    "    team_0_window_score_wins.append( 1 if team_0_score > 0 else 0)        \n",
    "\n",
    "    team_1_score = g_scores[GOALIE_1_KEY] + s_scores[STRIKER_1_KEY]\n",
    "    team_1_window_score.append( team_1_score )\n",
    "    team_1_window_score_wins.append( 1 if team_1_score > 0 else 0 )\n",
    "\n",
    "    draws.append( team_0_score == team_1_score )\n",
    "    \n",
    "    print('Episode {}'.format( episode + 1 ))\n",
    "    print('\\tRed Wins: \\t{} \\tScore: \\t{:.5f} \\tAvg: \\t{:.2f}'.format( np.count_nonzero(team_0_window_score_wins), team_0_score, np.sum(team_0_window_score) ))\n",
    "    print('\\tBlue Wins: \\t{} \\tScore: \\t{:.5f} \\tAvg: \\t{:.2f}'.format( np.count_nonzero(team_1_window_score_wins), team_1_score, np.sum(team_1_window_score) ))\n",
    "    print('\\tDraws: \\t{}'.format( np.count_nonzero( draws ) ))\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
